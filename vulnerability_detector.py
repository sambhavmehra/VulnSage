"""
AI-Powered Vulnerability Detector
Uses Groq AI and ML Model to intelligently detect and analyze vulnerabilities
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import parse_qs, urljoin, urlparse
import re
import joblib
import numpy as np
import pandas as pd
import os


class VulnerabilityDetector:
    """
    Scans web pages and uses AI to detect vulnerabilities
    """

    def __init__(
        self,
        enable_ai=True,
        max_pages=10,
        smart_crawl=True,
        model_path='vulnerability_model.pkl',
        enable_intel_model=True,
        intel_model_path='bug_intel_model.pkl',
        max_ai_validations=20,
    ):
        self.enable_ai = enable_ai
        self.max_pages = max_pages
        self.smart_crawl = smart_crawl
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Security Scanner)"
        }
        
        # Load ML model
        self.ml_model = None
        self.enable_ml = False
        
        try:
            if model_path and os.path.exists(model_path):
                self.ml_model = joblib.load(model_path)
                self.enable_ml = True
                print(f"[+] ML Model loaded successfully from {model_path}")
                print(f"[+] Model predicts: {', '.join(self.ml_model.classes_)}")
            else:
                print(f"[-] ML Model not found at {model_path}")
        except Exception as e:
            print(f"[-] Failed to load ML model: {e}")
            self.enable_ml = False

        # Load threat-intel self-trained model
        self.enable_intel_model = False
        self.intel_model = None
        self.intel_pipeline = None
        if enable_intel_model and intel_model_path and os.path.exists(intel_model_path):
            try:
                self.intel_model = joblib.load(intel_model_path)
                self.intel_pipeline = self.intel_model.get("pipeline") if isinstance(self.intel_model, dict) else None
                if self.intel_pipeline is not None:
                    self.enable_intel_model = True
                    print(f"[+] Threat-intel model loaded from {intel_model_path}")
            except Exception as e:
                print(f"[-] Failed to load threat-intel model: {e}")
        self.max_ai_validations = max_ai_validations

    def scan_target(self, target_url, groq_orchestrator):
        """
        Scan a target URL and detect vulnerabilities
        """
        print(f"\n[*] Scanning: {target_url}")

        vulnerabilities = []

        # Crawl the site
        pages = self._crawl_site(target_url)

        print(f"[*] Crawled {len(pages)} pages")

        # Analyze each page
        for page in pages:
            # ML Model prediction (NEW!)
            if self.enable_ml and self.ml_model:
                ml_vulns = self._ml_detect_vulnerabilities(page)
                vulnerabilities.extend(ml_vulns)

            if self.enable_intel_model and self.intel_pipeline:
                intel_vulns = self._intel_detect_vulnerabilities(page)
                vulnerabilities.extend(intel_vulns)
            
            # Traditional pattern-based detection
            traditional_vulns = self._detect_traditional_vulnerabilities(page)
            vulnerabilities.extend(traditional_vulns)

            # AI-powered detection
            if self.enable_ai and groq_orchestrator:
                ai_vulns = self._ai_detect_vulnerabilities(page, groq_orchestrator)
                vulnerabilities.extend(ai_vulns)

        # Add URL-based vulnerabilities
        vulnerabilities.extend(self._check_url_vulnerabilities(target_url))

        # Add header-based vulnerabilities
        vulnerabilities.extend(self._check_security_headers(target_url))

        print(f"[+] Found {len(vulnerabilities)} potential vulnerabilities")

        # AI validation of vulnerabilities
        if self.enable_ai and groq_orchestrator and vulnerabilities:
            validated_vulns = []
            validation_budget = self.max_ai_validations
            for idx, vuln in enumerate(vulnerabilities):
                if idx >= validation_budget:
                    # Keep remaining findings without AI validation to avoid hitting API limits.
                    validated_vulns.append(vuln)
                    continue
                validation = groq_orchestrator.validate_vulnerability(vuln)

                if validation.get('is_valid', True):
                    # Update vulnerability with validation results
                    vuln['severity'] = validation.get('refined_severity', vuln['severity'])
                    vuln['exploitation_difficulty'] = validation.get('exploitation_difficulty', 'Medium')
                    vuln['business_impact'] = validation.get('business_impact', 'Unknown')
                    vuln['risk_score'] = validation.get('risk_score', vuln.get('risk_score'))
                    validated_vulns.append(vuln)

            return self._normalize_and_enrich_vulnerabilities(validated_vulns)

        return self._normalize_and_enrich_vulnerabilities(vulnerabilities)

    def _crawl_site(self, start_url):
        """Crawl website pages"""
        visited = set()
        to_visit = [start_url]
        pages_data = []

        base_domain = urlparse(start_url).netloc

        while to_visit and len(visited) < self.max_pages:
            url = to_visit.pop(0)

            if url in visited:
                continue

            visited.add(url)

            try:
                response = requests.get(
                    url,
                    headers=self.headers,
                    timeout=10,
                    allow_redirects=True
                )

                if 'text/html' not in response.headers.get('Content-Type', ''):
                    continue

                soup = BeautifulSoup(response.text, 'html.parser')

                # Extract page data
                page_data = {
                    'url': url,
                    'html': response.text,
                    'status_code': response.status_code,
                    'headers': dict(response.headers),
                    'forms': self._extract_forms(soup),
                    'scripts': self._extract_scripts(soup),
                    'links': self._extract_links(soup, url),
                    'inputs': self._extract_inputs(soup),
                    'cookies': response.cookies.get_dict()
                }

                pages_data.append(page_data)

                # Smart crawling: prioritize interesting pages
                if self.smart_crawl:
                    interesting_links = [
                        link for link in page_data['links']
                        if self._is_interesting_page(link)
                    ]
                    to_visit.extend(interesting_links[:5])
                else:
                    # Add all internal links
                    for link in page_data['links']:
                        if urlparse(link).netloc == base_domain:
                            to_visit.append(link)

            except Exception as e:
                print(f"[-] Error crawling {url}: {e}")
                continue

        return pages_data

    def _is_interesting_page(self, url):
        """Determine if a page is interesting for security testing"""
        interesting_patterns = [
            'login', 'signin', 'auth', 'admin', 'dashboard',
            'upload', 'form', 'search', 'api', 'user', 'account',
            'payment', 'checkout', 'profile', 'settings'
        ]

        url_lower = url.lower()
        return any(pattern in url_lower for pattern in interesting_patterns)

    def _extract_forms(self, soup):
        """Extract all forms from page"""
        forms = []
        for form in soup.find_all('form'):
            forms.append({
                'action': form.get('action', ''),
                'method': form.get('method', 'GET').upper(),
                'inputs': [
                    {
                        'name': inp.get('name', ''),
                        'type': inp.get('type', 'text'),
                        'value': inp.get('value', '')
                    }
                    for inp in form.find_all('input')
                ]
            })
        return forms

    def _extract_scripts(self, soup):
        """Extract all scripts from page"""
        scripts = []
        for script in soup.find_all('script'):
            src = script.get('src', '')
            scripts.append({
                'src': src,
                'external': bool(src and ('http://' in src or 'https://' in src)),
                'inline': script.string if not src else None
            })
        return scripts

    def _extract_links(self, soup, base_url):
        """Extract all links from page"""
        links = []
        for link in soup.find_all('a', href=True):
            href = urljoin(base_url, link['href'])
            links.append(href)
        return links

    def _extract_inputs(self, soup):
        """Extract all inputs from page"""
        inputs = []
        for inp in soup.find_all('input'):
            inputs.append({
                'type': inp.get('type', 'text'),
                'name': inp.get('name', ''),
                'id': inp.get('id', '')
            })
        return inputs

    def _ml_detect_vulnerabilities(self, page):
        """
        Use ML model to predict vulnerabilities based on page features
        Model features: ['forms', 'scripts', 'missing_csp']
        Model classes: ['Misconfiguration', 'SQL Injection', 'XSS']
        """
        vulnerabilities = []
        url = page['url']
        
        try:
            # Extract features for the model (returns DataFrame)
            features_df = self._extract_ml_features(page)
            
            # Make prediction
            prediction = self.ml_model.predict(features_df)[0]
            probability = self.ml_model.predict_proba(features_df)[0]
            
            # Get confidence for the predicted class
            predicted_class_idx = list(self.ml_model.classes_).index(prediction)
            confidence = int(probability[predicted_class_idx] * 100)
            
            # Only report if confidence is above threshold
            if confidence >= 60:  # 60% confidence threshold
                
                # Map ML predictions to vulnerability details
                vuln_details = {
                    'SQL Injection': {
                        'type': 'ML-Detected SQL Injection Risk',
                        'severity': 'High',
                        'description': 'Machine learning model detected patterns indicating SQL injection vulnerability',
                        'recommendation': 'Use parameterized queries, implement input validation, and apply principle of least privilege to database accounts',
                        'cwe_id': 'CWE-89'
                    },
                    'XSS': {
                        'type': 'ML-Detected XSS Risk',
                        'severity': 'High',
                        'description': 'Machine learning model detected patterns indicating cross-site scripting vulnerability',
                        'recommendation': 'Implement output encoding, use Content Security Policy headers, and validate/sanitize all user inputs',
                        'cwe_id': 'CWE-79'
                    },
                    'Misconfiguration': {
                        'type': 'ML-Detected Security Misconfiguration',
                        'severity': 'Medium',
                        'description': 'Machine learning model detected security misconfigurations in the application',
                        'recommendation': 'Review security headers, implement CSP, ensure proper error handling, and follow security best practices',
                        'cwe_id': 'CWE-16'
                    }
                }
                
                vuln_info = vuln_details.get(prediction, {
                    'type': f'ML-Detected {prediction}',
                    'severity': 'Medium',
                    'description': f'Machine learning model detected potential {prediction}',
                    'recommendation': 'Manual security review recommended',
                    'cwe_id': 'CWE-693'
                })
                
                vulnerabilities.append({
                    'url': url,
                    'type': vuln_info['type'],
                    'severity': vuln_info['severity'],
                    'confidence': confidence,
                    'description': vuln_info['description'],
                    'location': 'ML Model Analysis',
                    'recommendation': vuln_info['recommendation'],
                    'cwe_id': vuln_info['cwe_id'],
                    'detection_method': 'Machine Learning',
                    'ml_prediction': prediction,
                    'ml_confidence_scores': {
                        cls: f"{prob*100:.1f}%" 
                        for cls, prob in zip(self.ml_model.classes_, probability)
                    }
                })
                
                print(f"[ML] Detected {prediction} with {confidence}% confidence on {url}")
        
        except Exception as e:
            print(f"[-] ML detection error on {url}: {e}")
        
        return vulnerabilities

    def _intel_detect_vulnerabilities(self, page):
        """
        Use self-trained threat-intel model to detect modern bug patterns.
        Conservative by default: only emits findings >= 80% confidence.
        """
        vulnerabilities = []
        url = page['url']
        try:
            text = BeautifulSoup(page.get('html', ''), 'html.parser').get_text(" ", strip=True)
            text = text[:6000]
            if not text:
                return vulnerabilities

            pred = self.intel_pipeline.predict([text])[0]
            confidence = 0
            if hasattr(self.intel_pipeline, "predict_proba"):
                proba = self.intel_pipeline.predict_proba([text])[0]
                confidence = int(max(proba) * 100)

            if confidence < 80:
                return vulnerabilities

            severity_map = {
                "RCE": "Critical",
                "SQL Injection": "High",
                "Auth Bypass": "High",
                "SSRF": "High",
                "XSS": "High",
                "Path Traversal": "Medium",
                "CSRF": "Medium",
                "Info Disclosure": "Medium",
                "Misconfiguration": "Medium",
            }
            cwe_map = {
                "RCE": "CWE-94",
                "SQL Injection": "CWE-89",
                "Auth Bypass": "CWE-287",
                "SSRF": "CWE-918",
                "XSS": "CWE-79",
                "Path Traversal": "CWE-22",
                "CSRF": "CWE-352",
                "Info Disclosure": "CWE-200",
                "Misconfiguration": "CWE-16",
            }

            label = str(pred)
            vulnerabilities.append({
                'url': url,
                'type': f'Intel-Model {label} Indicator',
                'severity': severity_map.get(label, 'Medium'),
                'confidence': confidence,
                'description': (
                    'Self-trained threat-intel model flagged this page as similar to recent '
                    f'public vulnerabilities ({label}).'
                ),
                'location': 'Threat-Intel Model Analysis',
                'recommendation': (
                    'Prioritize manual validation and targeted testing for this bug class before deployment.'
                ),
                'cwe_id': cwe_map.get(label, 'CWE-693'),
                'detection_method': 'Threat Intel Self-Training Model',
                'intel_prediction': label,
            })
        except Exception as e:
            print(f"[-] Threat-intel detection error on {url}: {e}")
        return vulnerabilities
    
    def _extract_ml_features(self, page):
        """
        Extract features required by the ML model
        Features: ['forms', 'scripts', 'missing_csp']
        """
        import pandas as pd
        
        # Feature 1: Number of forms
        num_forms = len(page['forms'])
        
        # Feature 2: Number of scripts
        num_scripts = len(page['scripts'])
        
        # Feature 3: Missing CSP (1 if missing, 0 if present)
        headers = page.get('headers', {})
        has_csp = 'Content-Security-Policy' in headers or 'content-security-policy' in headers
        missing_csp = 0 if has_csp else 1
        
        # Return as pandas DataFrame with proper feature names
        return pd.DataFrame([[num_forms, num_scripts, missing_csp]], 
                          columns=['forms', 'scripts', 'missing_csp'])

    def _detect_traditional_vulnerabilities(self, page):
        """Traditional pattern-based vulnerability detection"""
        vulnerabilities = []
        url = page['url']

        # Check for SQL Injection patterns
        if self._check_sql_injection_patterns(page):
            vulnerabilities.append({
                'url': url,
                'type': 'Potential SQL Injection',
                'severity': 'High',
                'confidence': 60,
                'description': 'Form inputs without proper sanitization detected',
                'location': 'Form inputs',
                'recommendation': 'Use parameterized queries and input validation',
                'cwe_id': 'CWE-89'
            })

        # Check for XSS vulnerabilities
        if self._check_xss_patterns(page):
            vulnerabilities.append({
                'url': url,
                'type': 'Potential Cross-Site Scripting (XSS)',
                'severity': 'High',
                'confidence': 55,
                'description': 'Unescaped user input or dangerous JavaScript patterns detected',
                'location': 'Page content / Scripts',
                'recommendation': 'Implement output encoding and Content Security Policy',
                'cwe_id': 'CWE-79'
            })

        # Check for CSRF
        if self._check_csrf_vulnerability(page):
            vulnerabilities.append({
                'url': url,
                'type': 'Missing CSRF Protection',
                'severity': 'Medium',
                'confidence': 70,
                'description': 'Forms without CSRF tokens detected',
                'location': 'Form elements',
                'recommendation': 'Implement CSRF tokens for all state-changing operations',
                'cwe_id': 'CWE-352'
            })

        # Check for sensitive data exposure
        if self._check_sensitive_data_exposure(page):
            vulnerabilities.append({
                'url': url,
                'type': 'Sensitive Data Exposure',
                'severity': 'Medium',
                'confidence': 65,
                'description': 'Potentially sensitive information found in page source',
                'location': 'HTML comments / Script variables',
                'recommendation': 'Remove sensitive data from client-side code',
                'cwe_id': 'CWE-200'
            })

        return vulnerabilities

    def _check_sql_injection_patterns(self, page):
        """Check for SQL injection vulnerability patterns"""
        # Check forms for SQL-injectable inputs
        for form in page['forms']:
            if form['method'] == 'GET':
                return True  # GET forms are more susceptible

            # Check for database-related inputs
            for inp in form['inputs']:
                if inp['type'] in ['text', 'search'] and not inp['name'].startswith('csrf'):
                    return True

        return False

    def _check_xss_patterns(self, page):
        """Check for XSS vulnerability patterns"""
        html = page['html']

        # Check for unescaped output patterns
        xss_patterns = [
            r'<script[^>]*>[^<]*</script>',
            r'eval\(',
            r'innerHTML\s*=',
            r'document\.write\(',
            r'on\w+\s*=\s*["\']'
        ]

        for pattern in xss_patterns:
            if re.search(pattern, html, re.IGNORECASE):
                return True

        return False

    def _check_csrf_vulnerability(self, page):
        """Check for CSRF vulnerability"""
        for form in page['forms']:
            if form['method'] == 'POST':
                # Check if form has CSRF token
                has_csrf_token = any(
                    'csrf' in inp['name'].lower() or 'token' in inp['name'].lower()
                    for inp in form['inputs']
                )

                if not has_csrf_token:
                    return True

        return False

    def _check_sensitive_data_exposure(self, page):
        """Check for sensitive data in page source"""
        html = page['html'].lower()

        sensitive_patterns = [
            'password', 'api_key', 'secret', 'token',
            'private_key', 'aws_', 'database', 'connection'
        ]

        # Check HTML comments
        comments = re.findall(r'<!--(.*?)-->', html, re.DOTALL)
        for comment in comments:
            if any(pattern in comment for pattern in sensitive_patterns):
                return True

        return False

    def _ai_detect_vulnerabilities(self, page, groq_orchestrator):
        """Use AI to detect vulnerabilities"""
        try:
            vulnerabilities = groq_orchestrator.analyze_page_content(
                url=page['url'],
                html_content=page['html'],
                forms_data=page['forms'],
                scripts_data=page['scripts']
            )

            # Add URL to each vulnerability
            for vuln in vulnerabilities:
                vuln['url'] = page['url']

            return vulnerabilities

        except Exception as e:
            print(f"[-] AI detection failed: {e}")
            return []

    def _check_url_vulnerabilities(self, url):
        """Check for URL-based vulnerabilities"""
        vulnerabilities = []
        parsed = urlparse(url)

        # Check for HTTP (not HTTPS)
        if url.startswith('http://'):
            vulnerabilities.append({
                'url': url,
                'type': 'Insecure Protocol (HTTP)',
                'severity': 'Medium',
                'confidence': 100,
                'description': 'Website uses HTTP instead of HTTPS',
                'location': 'Protocol',
                'recommendation': 'Implement HTTPS with valid SSL/TLS certificate',
                'cwe_id': 'CWE-319'
            })

        # Detect sensitive parameter names in URL query strings
        sensitive_params = {
            "token", "access_token", "auth", "apikey", "api_key", "key",
            "password", "passwd", "secret", "session", "jwt"
        }
        query_keys = {k.lower() for k in parse_qs(parsed.query).keys()}
        found_sensitive = sorted(query_keys.intersection(sensitive_params))
        if found_sensitive:
            vulnerabilities.append({
                'url': url,
                'type': 'Sensitive Data in URL Parameters',
                'severity': 'High',
                'confidence': 85,
                'description': (
                    f"Sensitive query parameters exposed in URL: {', '.join(found_sensitive)}"
                ),
                'location': 'URL Query String',
                'recommendation': 'Move sensitive values to secure headers or POST body and rotate exposed secrets',
                'cwe_id': 'CWE-598'
            })

        return vulnerabilities

    def _check_security_headers(self, url):
        """Check for missing security headers"""
        vulnerabilities = []

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            headers = response.headers

            # Check for important security headers
            security_headers = {
                'X-Frame-Options': 'Medium',
                'X-Content-Type-Options': 'Low',
                'Strict-Transport-Security': 'Medium',
                'Content-Security-Policy': 'Medium',
                'X-XSS-Protection': 'Low'
            }

            for header, severity in security_headers.items():
                if header not in headers:
                    vulnerabilities.append({
                        'url': url,
                        'type': f'Missing Security Header: {header}',
                        'severity': severity,
                        'confidence': 95,
                        'description': f'{header} header not set',
                        'location': 'HTTP Response Headers',
                        'recommendation': f'Add {header} header to HTTP responses',
                        'cwe_id': 'CWE-16'
                    })

        except:
            pass

        return vulnerabilities

    def _normalize_and_enrich_vulnerabilities(self, vulnerabilities):
        """
        Normalize, de-duplicate, and rank findings to reduce noisy output.
        """
        severity_map = {
            "critical": "Critical",
            "high": "High",
            "medium": "Medium",
            "low": "Low",
            "info": "Info",
            "informational": "Info",
        }
        severity_weight = {"Critical": 95, "High": 75, "Medium": 55, "Low": 35, "Info": 15}

        dedup = {}
        for vuln in vulnerabilities:
            if not isinstance(vuln, dict):
                continue

            severity = str(vuln.get("severity", "Medium")).strip().lower()
            normalized_severity = severity_map.get(severity, "Medium")
            confidence = vuln.get("confidence", 50)
            try:
                confidence = int(confidence)
            except (TypeError, ValueError):
                confidence = 50
            confidence = max(0, min(100, confidence))

            if vuln.get("risk_score") is None:
                base = severity_weight.get(normalized_severity, 50)
                vuln["risk_score"] = min(100, int(base + (confidence * 0.1)))

            vuln["severity"] = normalized_severity
            vuln["confidence"] = confidence
            vuln.setdefault("location", "Unknown")
            vuln.setdefault("recommendation", "Manual security review recommended")
            vuln.setdefault("description", "Potential security issue detected")

            key = (
                vuln.get("url", ""),
                vuln.get("type", ""),
                vuln.get("location", ""),
                vuln.get("description", ""),
            )

            existing = dedup.get(key)
            if not existing or vuln["confidence"] > existing.get("confidence", 0):
                dedup[key] = vuln

        ranked = sorted(
            dedup.values(),
            key=lambda v: (int(v.get("risk_score", 0)), int(v.get("confidence", 0))),
            reverse=True,
        )
        return ranked
